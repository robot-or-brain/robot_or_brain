{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/robot-or-brain/robot_or_brain/blob/main/robotorbrain_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "def download(url):\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "    os.system(f\"wget {url}\")\n",
    "    return filename"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "robot_image_path = download('https://www.evolving-science.com/sites/default/files/field/image/Artificial-Intelligence.jpg')\n",
    "helping_image_path = download(\"https://www.ingenious.news/wp-content/uploads/2019/08/ai.png\")\n",
    "scanning_image_path = download('https://b360nepal.com/wp-content/uploads/2021/01/techmind.jpg')\n",
    "traffic_image_path = download('http://pythonawesome.com/content/images/2020/03/Object-Detection-and-Tracking.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "_collab = (\"Collaborative/ Interactive AI // Human-computer interaction\", \"\"\"Collaborative/ Interactive AI // Human-computer interaction\n",
    "= AI depicted as interacting with humans, in the form of robots, human-computer interfaces, AI as a recognition device, or entering a relationship with humans, AI performs a task that aids or supports humans. AI and humans are depicted at one level, not one dominating the other (as opposed to the superior human frame).\n",
    " Included concepts:\n",
    "−\tWelfare\n",
    "−\tJob producer\n",
    "−\tProgress\n",
    "−\tMoral agent\n",
    "Examples from coding:\"\"\")\n",
    "collab = \"AI interacting with humans, AI performs a task that supports humans.\"\n",
    "\n",
    "_thinking = \"\"\"Thinking machine = AI embodied as the “thinking machine” visualized as a brain, face, eye, android or the like that shows to process or store large amounts of data like a machine (depicted, amongst others, with gears, wheels, wires, data points, etc.). “Thinking” refers to processes going on in a brain/head that are visualized, e.g., as wires, data points. Do not code if appeals to “magic AI.” \"\"\"\n",
    "thinking = \"AI visualized as a brain, face, eye, android storing large amounts of data\"\n",
    "len(thinking)\n",
    "\n",
    "_acting = \"\"\"Acting/Performing machine\n",
    "= can be impersonated or in the form of a machine/robot/product, visualizes AI as performing an act or task, just as (or even more) capable than humans, in a non-threatening (as in menacing machine) manner. In this frame, no people are present.\n",
    "Included concepts:\n",
    "−\tTrust\n",
    "−\tLikeable AI\n",
    "−\tFriendly AI\n",
    "−\tAI as game\"\"\"\n",
    "acting = \"Machine,robot or product, performing an act or task\"\n",
    "\n",
    "_learning = \"\"\"Learning/recognition machine // AI as software // Machine learning\n",
    "= visualization of AI as scanning or recognizing data points in an image, for instance, traffic, shops, or faces. If a human is the subject of scanning, this frame applies, too. However, if the scanning/recognition is part of an interaction between humans, this is coded as collaborative AI\n",
    "Included concepts:\n",
    "−\tProgress\n",
    "−\t4th industrial revolution\"\"\"\n",
    "learning = \"AI scanning or recognizing data points in traffic, shops, or faces\"\n",
    "\n",
    "_mysterious = \"\"\"Mysterious AI\n",
    "= depicts AI as working “magically” or in a mysterious way. This can be visualized as a shining light or as a scene where the consequence of the activity is unknown. It can also be visualized as creating something new, as being god-like, or to establish connections between the human and computer that are depicted as “magic” (as opposed to working together like in collaborative AI).\n",
    "Included concepts:\n",
    "−\tMagic powers\n",
    "−\t“blessed by the algorithm”\n",
    "−\tDeification\n",
    "−\tDominance\n",
    "−\t“good” AI\n",
    "−\tCreation\n",
    "−\tImmortality\n",
    "−\tQuasi-religious\n",
    "−\tMetaphysical machine\"\"\"\n",
    "mysterious = \"AI working “magically” or in a mysterious way.\"\n",
    "\n",
    "_complx = \"\"\"Complex AI\n",
    "= AI visualized as complex data, complex interactions/nodes/networks, also in cities, the earth, or a complex technology that works dynamically. Only code if complexity is a salient part of the image.\n",
    "Included concepts:\n",
    "−\tComplexity\n",
    "−\tNonlinearity\"\"\"\n",
    "complx = \"AI visualized as complex data, complex interactions/nodes/networks\"\n",
    "\n",
    "_superior = \"\"\"Superior human\n",
    "= AI depicted as technology that is created and/or controlled by humans. Do not code magic AI if the feature that humans control AI is salient.\n",
    "−\tGratification\n",
    "−\tEase\n",
    "−\tAI for humanity\n",
    "−\tDominance (of humans, not AI)\"\"\"\n",
    "superior = \"AI depicted as technology that is created and/or controlled by humans.\"\n",
    "\n",
    "frames = [collab, thinking, acting, learning, mysterious, complx, superior]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "base_dir = Path('../robot-or-brain-data/i')\n",
    "split = 'validation'\n",
    "validation_dir = base_dir / split\n",
    "\n",
    "classes = [p.name for p in validation_dir.iterdir()]\n",
    "current_class = classes[1]\n",
    "class_dir = validation_dir / current_class\n",
    "print(class_dir)\n",
    "image_path = [p for p in class_dir.iterdir()][6]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n          ...,\n          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n          [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303]],\n\n         [[2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n          ...,\n          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n          [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749]],\n\n         [[2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n          ...,\n          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n          [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459]]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "img = Image.open('../portrait_chris.PNG')\n",
    "image = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "# model.encode_image(image)\n",
    "# labels = [\"a helpful artificial intelligence\", \"Thinking machine\", \"Acting/performing machine\", \"A person walking his dog in a crowded area in a city.\", \"a cat\",\"a monster\",\"a floating brain\", \"Complex AI\"] + frames\n",
    "# text = clip.tokenize(labels).to(device)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     # image_features = model.encode_image(image)\n",
    "#     # text_features = model.encode_text(text)\n",
    "#\n",
    "#     logits_per_image, logits_per_text = model(image, text)\n",
    "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "#\n",
    "#\n",
    "# for prob, label in sorted(zip(probs[0], labels), key=lambda x : x[0], reverse=True):\n",
    "#     print(f'{prob:.2f} {label}')\n",
    "# img\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 224, 224])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (7) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m a \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m a\u001B[38;5;241m.\u001B[39mshape\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\robot\\lib\\site-packages\\clip\\model.py:342\u001B[0m, in \u001B[0;36mCLIP.encode_image\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode_image\u001B[39m(\u001B[38;5;28mself\u001B[39m, image):\n\u001B[1;32m--> 342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\robot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\robot\\lib\\site-packages\\clip\\model.py:228\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    226\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# shape = [*, width, grid ** 2]\u001B[39;00m\n\u001B[0;32m    227\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# shape = [*, grid ** 2, width]\u001B[39;00m\n\u001B[1;32m--> 228\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_embedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, x], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# shape = [*, grid ** 2 + 1, width]\u001B[39;00m\n\u001B[0;32m    229\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_embedding\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    230\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_pre(x)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (768) must match the size of tensor b (7) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a = model.encode_image(image[0])\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMgmnWTR++y/296Ib++LJIw",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "robotorbrain_clip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}